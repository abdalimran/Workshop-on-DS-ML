{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Introductory Data Science & Machine Learning</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personal Branding [SHAMELESS!!]\n",
    "\n",
    "* **Name**: Abdullah Al Imran\n",
    "* **Currently**: Data Science Trainer at Divine IT Ltd.\n",
    "* **Previously**:\n",
    "    * Research Assistant at Data & Design Lab, Dept. of CS, University of Dhaka. *[1.5 year]*\n",
    "    * Data Scienist at Revvey Co, a New York based start-up. *[4 months]*\n",
    "* **Education**: BSc. in Computer Science & Engineering, American Internaional University-Bangladesh, [14-1]\n",
    "* **Connect** with me on [LinkedIn](https://www.linkedin.com/in/abdalimran/), [Facebook](https://www.facebook.com/abdalimran)\n",
    "* **Email** me at [abdalimran@gmail.com]() [I'm very active in email!!]\n",
    "* **Website:** http://abdalimran.github.io\n",
    "* **Recent recognition:** 1st Runner-Up at Robi-Axita Datathon 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "* [Motivation](#motivation)\n",
    "    * An overview of Data Scales\n",
    "    * A Day in Data\n",
    "    * Why get into Data Science?\n",
    "* [Data Science: Birds Eye View](#data_science_view)\n",
    "    * What is Data Science?\n",
    "    * A bit of history?\n",
    "    * The Data Science Venn Diagram\n",
    "    * What skills do you need to be a Data Scientist?\n",
    "* [Machine Learning](#machine_learning)\n",
    "    * What is Machine Learning?\n",
    "    * Traditional Programming vs Machine Learning\n",
    "    * Artificial Intelligence vs Machine Learning vs Deep Learning vs Data Mining\n",
    "    * Top Industrial Applications of Machine Learning\n",
    "    * Branches of Machine Learning\n",
    "* [The Data Science Pipeline](#ds_pipeline)\n",
    "    * Collection\n",
    "    * Exploration\n",
    "    * Munging\n",
    "    * Modeling\n",
    "    * Validation\n",
    "    * Communication\n",
    "* [Case Study 1: Regression](#case1)\n",
    "    * Problem Description\n",
    "    * Solution\n",
    "* [Case Study 2: Classification](#case2)\n",
    "    * Problem Description\n",
    "    * Solution\n",
    "* [Case Study 3: Clustering](#case3)\n",
    "    * Problem Description\n",
    "    * Solution\n",
    "* [Learning Resources](#resources)\n",
    "* [Conducting research in Applied Data Science and Machine Learning](#research)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='motivation'></a>\n",
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An overview of Data Scales\n",
    "\n",
    "|Abbreviation|Unit|Value|Size (in bytes)|\n",
    "|--- |--- |--- |--- |\n",
    "|b|bit|0 or 1|1/8 of a byte|\n",
    "|B|bytes|8 bits|1 byte|\n",
    "|KB|kilobytes|1,000 bytes|1,000 bytes|\n",
    "|MB|megabyte|1,000² bytes|1,000,000 bytes|\n",
    "|GB|gigabyte|1,000³ bytes|1,000,000,000 bytes|\n",
    "|TB|terabyte|1,000⁴ bytes|1,000,000,000,000 bytes|\n",
    "|PB|petabyte|1,000⁵ bytes|1,000,000,000,000,000 bytes|\n",
    "|EB|exabyte|1,000⁶ bytes|1,000,000,000,000,000,000 bytes|\n",
    "|ZB|zettabyte|1,000⁷ bytes|1,000,000,000,000,000,000,000 bytes|\n",
    "|YB|yottabyte|1,000⁸ bytes|1,000,000,000,000,000,000,000,000 bytes|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Day in Data\n",
    "Data is growing exponentially!! Let's look at this interestring infographic by RACONTEUR.\n",
    "\n",
    "<img src=\"imgs/a-day-in-data.jpg\" alt=\"drawing\" width=\"1000\">\n",
    "\n",
    "By 2025, it’s estimated that 463 exabytes of data will be created each day globally – that’s the equivalent of 212,765,957 DVDs per day!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why get into Data Science?\n",
    "* You'll be wise, logical, knowledgable and a better human because, **Data → Information → Knowledge → Wisdom**\n",
    "* Data Science will give you the joy of discovery and the power of predicting the future!!\n",
    "* You'll never get bored! Every Data Science problem is a new roller-coaster ride!!\n",
    "* There is no specific application domain for Data Science. Where there is data, there is Data Science. You just need to imagine!\n",
    "* Data is growing exponentially. The world needs data talents to make use of the huge amount of data.\n",
    "* The January 2019 report from Indeed, one of the top job sites, showed a 29% increase in demand for data scientists year over year and a 344% increase since 2013 -- a dramatic upswing. But while demand -- in the form of job postings -- continues to rise sharply, searches by job seekers skilled in data science grew at a slower pace (14%), suggesting a gap between supply and demand.\n",
    "* According to an annual Dice Salary Survey, the role of data scientist carries an average salary of $106,000 a year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So, are you convinced??**\n",
    "\n",
    "Let the journey begin!!\n",
    "\n",
    "<img src=\"imgs/roller-coaster-ride.jpg\" alt=\"drawing\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_science_view'></a>\n",
    "## Data Science: Birds Eye View"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Data Science?\n",
    "Data science is a **multi-disciplinary** field that uses scientific methods, processes, algorithms and systems to **extract knowledge** and insights from structured and unstructured **data**.\n",
    "\n",
    "#### A bit of history?\n",
    "* In **1962**, **John W. Tukey** wrote **\"The Future of Data Analysis\"** where he told that data analysis is intrinsically an empirical science.\n",
    "\n",
    "* In **1974**, **Peter Naur** published the book **Concise Survey of Computer Methods** in Sweden and the United States. The term **\"data science\"** has been used freely in this book. Naur also offered a definition of data science: *\"The science of dealing with data, once they have been established, while the relation of the data to what they represent is delegated to other fields and sciences.\"*\n",
    "\n",
    "* Later, in **1977**, Tukey published **Exploratory Data Analysis**, arguing that more emphasis needed to be placed on using data to suggest hypotheses to test.\n",
    "\n",
    "* In **1989**, **Gregory Piatetsky-Shapiro** organizes and chairs the first **Knowledge Discovery in Databases (KDD)** workshop. In 1995, it became the annual **ACM SIGKDD** Conference on Knowledge Discovery and Data Mining (KDD).\n",
    "\n",
    "* In **1996**, the members of the International Federation of Classification Societies (IFCS) meet in Kobe, Japan, for their biennial conference. For the first time, the term **\"data science\"** was included in the title of the conference (“Data science, classification, and related methods”).\n",
    "\n",
    "* In **1997**, The journal **Data Mining and Knowledge Discovery** was launched.\n",
    "\n",
    "* Around **2007**, Turing award winner **Jim Gray** envisioned \"data-driven science\" as a **\"fourth paradigm\"** of science that uses the computational analysis of large data as primary scientific method.\n",
    "\n",
    "* ..................................................................\n",
    "* ..................................................................\n",
    "* ..................................................................\n",
    "\n",
    "* Finally, data science made its BIG BANG in **2012** when **Tom Davenport and D.J. Patil** published an article titled **\"Data Scientist: The Sexiest Job of the 21st Century\"** in the Harvard Business Review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Data Science Venn Diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/data_science_venn_diagram.png\" alt=\"drawing\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What skills do you need to be a Data Scientist?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Mathematics**: Linear Algebra, Calculus (Univariate, Multivariate), Numerical Optimization\n",
    "* **Statistics**: Descriptive Stats, Inferential Stats, Probability Theory, Bayesian\n",
    "* **Programming**: Python or R or Julia\n",
    "* **Machine Learning**: Regression, Classification, Clustering, NLP\n",
    "* **Database**: SQL, NoSQL\n",
    "* **Domain Expertise**\n",
    "* Obviously **MOTIVATION** and **GRIT**!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='machine_learning'></a>\n",
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think about Machine Learning? Is it as following?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/ml.png\" alt=\"drawing\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning is a **branch** of artificial intelligence (AI) that provides systems the ability to **automatically** learn and **improve from experience** without being **explicitly** programmed.\n",
    "\n",
    "Machine learning focuses on the development of computer programs that can access data and use it learn for themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional Programming vs Machine Learning\n",
    "\n",
    "<img src=\"imgs/supervised_learning_vs_programming.jpg\" alt=\"drawing\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Intelligence vs Machine Learning vs Deep Learning vs Data Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Artificial Intelligence:** Artificial Intelligence (AI) is the **general** field of computer\n",
    "science that has to do with creating programs that **simulate human intelligence**.\n",
    "\n",
    "**Machine Learning:** Machine Learning (ML) is a **subfield** of AI that focuses on\n",
    "making programs that can learn **without being explicitly programmed**.\n",
    "\n",
    "**Deep Learning:** Deep Learning focuses on a **specific** ML algorithm, **Neural\n",
    "Networks (NN)**, and more specifically Deep NNs(NNs with many layers).\n",
    "\n",
    "**Data Mining:** Data Mining is a field of computer science that **discovers patterns**\n",
    "through **mining** in data, and come up with **insights**. Here, the data is mostly\n",
    "raw/unstructured data. Data mining uses techniques and algorithms from machine\n",
    "learning, statistics and database theory to mine large databases and come up with\n",
    "patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Industrial Applications of Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Financial Services\n",
    "\n",
    "* Risk analytics and regulation\n",
    "* Customer Segmentation\n",
    "* Cross-selling and up-selling\n",
    "* Sales and marketing campaign management\n",
    "* Credit worthiness evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Virtual Personal Assistants (AI/ML)\n",
    "\n",
    "* Amazon’s Alexa\n",
    "* Google Assistant\n",
    "* Apple’s Siri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Marketing and Sales\n",
    "\n",
    "* Price optimization\n",
    "* Customer segmentation\n",
    "* Promotional offers\n",
    "* Allocation of budget\n",
    "* Personalize customer experience\n",
    "* Supply & demand analysis\n",
    "* Insights into customer’s emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Travel and Hospitality\n",
    "\n",
    "* Aircraft scheduling\n",
    "* Dynamic pricing\n",
    "* Sonsumer feedback and interaction analysis\n",
    "* Customer complaint resolution\n",
    "* Traffic patterns and congestion management\n",
    "* Ride sharing\n",
    "* Online ticketing, booking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Healthcare and Life Sciences\n",
    "\n",
    "* Alerts and diagnostics from real-time patient data\n",
    "* Disease Identification and risk stratification\n",
    "* Patient triage optimization\n",
    "* Proactive health management\n",
    "* Healthcare provider sentiment analysis\n",
    "* DNA Protein Sequence Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Social Media Services\n",
    "\n",
    "* Friend suggestion\n",
    "* Photo tagging and recognition\n",
    "* Social media marketing\n",
    "* Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Branches of Machine Learning\n",
    "\n",
    "Typically Machine Learning has 3 branches.\n",
    "\n",
    "**1. Supervised Learning**: You have a **target, a value or a class to predict**. For instance, let’s say you want to predict the revenue of a store from different inputs (day of the week, advertising, promotion). Then your model will be trained on historical data and use them to forecast future revenues. Hence the model is **supervised**, it knows what to learn. Supervised learning includes regression and classification problems.\n",
    "\n",
    "\n",
    "**2. Unsupervised Learning**: You have **unlabelled data and looks for patterns, groups** in these data. For example, you want to cluster to clients according to the type of products they order, how often they purchase your product, their last visit, … Instead of doing it manually, **unsupervised** machine learning will automatically discriminate different clients. Unsupervised learning includes clustering, anomaly detection etc.\n",
    "\n",
    "\n",
    "**3. Reinforcement Learning**: You want to **attain an objective**. In reinforcement learning, you have an environment and an objective to attain based on some rules. Now, you'll create a software **agent** that will take **actions** in the environment according to the **rules** so as to maximize some cumulative **reward**. For example, you want to find the best strategy to win a game with specified rules. Once these rules are specified, **reinforcement** learning techniques will play this game many times to find the best strategy. Some examples of reinforcement learning are - Google Alpha Go, Self Driving Cars etc.\n",
    "\n",
    "However, in modern machine learning, the branches got some reorder. Look at the following map.\n",
    "\n",
    "<img src=\"imgs/ml_branches.jpg\" alt=\"drawing\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ds_pipeline'></a>\n",
    "## The Data Science Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/data_pipeline.png\" alt=\"drawing\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Collection\n",
    "* Collect historical/relevent data for your problem from reliable sources. Sometime may be from heterogeneous (multiple) sources.\n",
    "\n",
    "#### 2. Exploration\n",
    "* Perform exploratory statistical analysis (EDA) to understand every bit of your data. \n",
    "* Use different visualization techniques and descriptive statistics.\n",
    "\n",
    "#### 3. Preprocessing\n",
    "* This is the most tedious and important part of any data science project. \n",
    "* You need to perform - \n",
    "    * Data cleaning, \n",
    "    * Missing value handling, \n",
    "    * Feature encoding, \n",
    "    * Normalization/Scaling/Standardization\n",
    "    * Feature selection, \n",
    "    * Feature engineering, and \n",
    "    * Data segregation \n",
    "* Efforts put in this phase will highly affect your ultimate results.\n",
    "\n",
    "#### 4. Modeling\n",
    "* This is the Machine Learning part! YESSS!! 😃\n",
    "* Choose the right algorithm for your data.\n",
    "* Find the best hyperparameters for your algorithm by tuning on the data.\n",
    "* Be careful!! DON'T **OVERFIT** YOUR MODEL! ⚠️\n",
    "\n",
    "#### 5. Validation\n",
    "* Mostly called *Testing*.\n",
    "* Choose the correct metric for the validation of your model.\n",
    "* Analyse the training and validation performance and see if the model is overfitting or not. Is the result close/consistent?\n",
    "\n",
    "#### 6. Communication\n",
    "* Interpret your model.\n",
    "* Discover the hidden patterns.\n",
    "* Make documentation.\n",
    "* Deploy your model to production.\n",
    "\n",
    "**An interesting stats on the time expenditure of a Data Scientist on different sections of the data science pipeline.**\n",
    "\n",
    "<img src=\"imgs/time-doing.png\" alt=\"drawing\" width=\"2000\">\n",
    "\n",
    "<h5 style=\"color:red;\">**We will try to follow the pipeline in the upcoming case studies.</h5>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='case1'></a>\n",
    "# Case Study 1: Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=color:navy>Problem Description:</h3>\n",
    "\n",
    "A reputed mobile company is launching it's new smartphone. The company is confused how they should set the price for the new smartphone. They need to ensure that the price is properly optimized as well as attractive to the consumers. They have historical data of there previous smartphone with market prices. Now, the dataset has been given to you which contains several important features of the previously released phones and their market prices. Using the parameters of the historical data you need to build a model that can predict an otimized price for the upcoming new smartphone.\n",
    "\n",
    "#### Let's CRACK it!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "data_rgr = pd.read_csv(\"data/regression_mobile_price.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Exploration (Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have glance of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_rgr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the data shape and datatypes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "No of columns: {}\n",
    "No of rows: {}\n",
    "No of categorical columns: {}\n",
    "No of numerical columns: {}\"\"\".format(data_rgr.shape[1], \n",
    "                                      data_rgr.shape[0],\n",
    "                                      len(data_rgr.select_dtypes('O').columns),\n",
    "                                      len(data_rgr.select_dtypes(['int', 'float']).columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a descriptive statistical tables of the numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_cols = data_rgr.select_dtypes(['int', 'float']).columns\n",
    "data_rgr[num_cols].describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How's the distribution of numerical columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def plot_distribution(data, features):\n",
    "    i = 0\n",
    "    plt.figure()\n",
    "    col = 3\n",
    "    row = int(np.ceil(len(features)/col))\n",
    "    fig, ax = plt.subplots(row,col,figsize=(18,20))\n",
    "\n",
    "    for feature in features:\n",
    "        i += 1\n",
    "        plt.subplot(row,col,i);\n",
    "        sns.distplot(tuple(data[feature]))\n",
    "        plt.xlabel(feature, fontsize=16)\n",
    "        locs, labels = plt.xticks()\n",
    "        plt.tick_params(axis='x', which='major', labelsize=12)\n",
    "        plt.tick_params(axis='y', which='major', labelsize=12)\n",
    "    plt.show();\n",
    "    \n",
    "plot_distribution(data_rgr, num_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there any correlation between the numeric columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = data_rgr[num_cols].corr()\n",
    "fig, ax = plt.subplots(figsize=(10,8))  \n",
    "sns.heatmap(corr, \n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values,\n",
    "            annot=True, fmt=\".2f\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the categorical columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = data_rgr.select_dtypes('O').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_plot(data, features):\n",
    "    i = 0\n",
    "    plt.figure()\n",
    "    col = 2\n",
    "    row = int(np.ceil(len(features)/col))\n",
    "    fig, ax = plt.subplots(row,col,figsize=(12,8))\n",
    "\n",
    "    for feature in features:\n",
    "        i += 1\n",
    "        plt.subplot(row,col,i);\n",
    "        ax = sns.countplot(data=data_rgr,y=feature)\n",
    "        ax.set_yticklabels(ax.get_yticklabels())\n",
    "        plt.tight_layout()\n",
    "        plt.ylabel(feature, fontsize=14)\n",
    "    plt.show();\n",
    "    \n",
    "count_plot(data_rgr, cat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there any **missing values**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(data_rgr.isnull().sum(), columns=['Missing Values'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to encode the **categorical features**.\n",
    "\n",
    "**Note**:\n",
    "* If you use any linear/distance/neural based model, use **OneHotEncoding**.\n",
    "* If you use tree based model, you can use **LabelEncoder**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rgr_encoded = pd.get_dummies(data_rgr, columns=cat_cols, prefix=cat_cols)\n",
    "\n",
    "print(\"Now our dataset has {} columns.\".format(data_rgr_encoded.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_rgr_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time for **data segregation**. We need to separate the feature columns and target colum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_rgr_encoded.loc[:, data_rgr_encoded.columns!='approx_price_EUR']\n",
    "y = data_rgr_encoded.loc[:, data_rgr_encoded.columns=='approx_price_EUR']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen from the distribution plot (in Exploration section) that most of the features are not well distributed and have variety of ranges. This form of data can confuse the linear/distance based models. That is why we need **Normalize** our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "scaler = Normalizer()\n",
    "scaler.fit(X.values)\n",
    "X_norm = scaler.transform(X.values)\n",
    "X_norm_df = pd.DataFrame(X_norm, columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(X_norm_df, num_cols[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to make dataset for **traing and testing**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_norm_df, y, test_size=0.20, random_state=1234)\n",
    "\n",
    "print(\"\"\"\n",
    "X_train has {} data points.\n",
    "y_train has {} data points.\n",
    "X_test has {} data points.\n",
    "y_test has {} data points.\n",
    "\"\"\".format(X_train.shape[0], y_train.shape[0], X_test.shape[0], y_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WOOOHAAA!! Here it is!\n",
    "\n",
    "Finally, we've reached the modeling part! The easiest one!\n",
    "\n",
    "We'll use **Linear Regression** algorithm for data modeling. This is the simplest, easiest and most utilized machine learning algorithms in the world!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression performs the task to predict a dependent variable value **(y)** based on a given independent variable **(x)**. So, this regression technique finds out a linear relationship between **x (input)** and **y (output)**. Hence, the name is Linear Regression.\n",
    "\n",
    "The mathematical formula of the linear regression can be written as follow:\n",
    "\n",
    "## <center>$y = \\beta_0+\\beta_1x+\\varepsilon$</center>\n",
    "\n",
    "We read this as $y$ is modeled as $beta_1$ ($\\beta_1$) times $x$, plus a constant $beta_0$ ($\\beta_0$), plus an error term $\\varepsilon$.\n",
    "\n",
    "Here:\n",
    "* $\\beta_0$ is the intercept\n",
    "* $\\beta_1$ is the regression weights or coefficients associated with the predictors $x$\n",
    "* $\\varepsilon$ is the error term (also known as the residual errors)\n",
    "\n",
    "The figure below illustrates a simple linear regression model, where:\n",
    "\n",
    "<img src=\"imgs/linear-regression.png\" alt=\"drawing\" width=\"400\">\n",
    "\n",
    "From the scatter plot above, it can be seen that not all the data points fall exactly on the fitted regression line. Some of the points are above the blue line and some are below it; overall, the residual errors ($e$) have approximately mean zero.\n",
    "\n",
    "The sum of the squares of the residual errors are called the **Residual Sum of Squares or RSS**.\n",
    "Mathematically, the beta coefficients ($\\beta_0$ and $\\beta_1$) are determined so that the RSS is as minimal as possible. This method of determining the beta coefficients can be **ordinary least squares (OLS)** or **gradient descent.**\n",
    "\n",
    "\n",
    "**Linear Regression has 2 of the following types:**\n",
    "1. Simple Linear Regression\n",
    "2. Multiple Linear Regression\n",
    "\n",
    "When you have a single predictor or independent variable, it is **simple linear regression**. The above equation we saw was the equation of simple linear regression.\n",
    "\n",
    "In **multiple linear regression** you have multiple predictor variables. \n",
    "The equation will be:\n",
    "\n",
    "## <center>$y = \\beta_0+\\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n + \\varepsilon$ </center>\n",
    "\n",
    "Here:\n",
    "* $\\beta_0$ is the intercept,\n",
    "* $\\beta_1, \\beta_2, \\beta_n$ are the regression weights or coefficients associated with the predictors $x_1, x_2, x_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's train a Linear Regression to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are wide variety of evaluation metrics for validating regression models such as **Mean absolute error, Mean squared error, Mean squared logarithmic error, Median absolute error regression,\t$R^2$ score**. For our problem we'll use the $R^2$ metric. The close the $R^2$ score to 1, the better the model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2 = -r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"The R^2 score is {}\".format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make prediction on live data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "live_data_regr = pd.DataFrame({\n",
    "  'weight_g': 260.0,\n",
    "  'SIM': 'Dual',\n",
    "  'display_type': 'IPS',\n",
    "  'display_resolution': 7.0,\n",
    "  'display_size_ppi': 210,\n",
    "  'OS': 'Marshmallow',\n",
    "  'CPU': 'Quad-core',\n",
    "  'memory_card': 128,\n",
    "  'internal_memory_GB': 16,\n",
    "  'RAM_GB': 2.0,\n",
    "  'primary_camera': 13.0,\n",
    "  'secondary_camera': 2.0,\n",
    "  'battery': 3400\n",
    "},index=[0])\n",
    "\n",
    "live_data_regr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "live_data_regr_encoded = live_data_regr.reindex(columns=X_test.columns, fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "live_data_regr_norm = scaler.transform(live_data_regr_encoded.values)\n",
    "live_data_regr_norm_df = pd.DataFrame(live_data_regr_norm, columns=live_data_regr_encoded.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make prediction using the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_live = lr_model.predict(live_data_regr_norm_df)\n",
    "print(y_pred_live)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Intercept of the Regression model:', -lr_model.intercept_[0])\n",
    "print('\\nSlope/Coefficients of the Regression model:')\n",
    "slope = pd.DataFrame(lr_model.coef_.T, X_train.columns, columns=['Coefficient']).sort_values(by='Coefficient', ascending=False)\n",
    "slope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='case2'></a>\n",
    "# Case Study 2: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "<h3 style=color:navy>Problem Description:</h3>\n",
    "\n",
    "Have you ever heared about the job \"**Wine Testing**\"? \n",
    "\n",
    "<img src=\"imgs/wine_tester.jpg\" alt=\"drawing\" width=\"500\">\n",
    "\n",
    "While a degree is not required to become a wine taster, it is difficult to land a top job without having some training in wine. Wine tasters work in wineries, bars, for magazines and even in hotels and restaurants. According to an article by Kathleen Green on the Bureau of Labor Statistics website, it is estimated that a wine tester (master sommelier) can earn as much as **\\$160,000 a year**. Simply Hired estimates that less-experienced wine tasters make an average of **\\$71,000** a year as of 2012.\n",
    "\n",
    "The wine production companies don't want to pay such a big ammount to the wine testers anymore. They have a good collection of their previous wine quality data with ratings in a range of $0-2$. Now, they want you to build a model that can accurately predict ratings for a newly produced wine using the previous parameters.\n",
    "\n",
    "**Note:** Here, the target is to predict a rating. Rating has a range of $0-2$. It is an ordinal categorical variable. Here, **0=Bad Quality, 1=Good Quality, and 2=Best Quality**. So, we can approch the problem as a classification problem.\n",
    "\n",
    "#### Let your model HACK the job of wine testers!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_clsf = pd.read_csv(\"data/classification_wine_quality.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Exploration (Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a glance of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_clsf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the data shape and datatypes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "No of columns: {}\n",
    "No of rows: {}\n",
    "No of categorical columns: {}\n",
    "No of numerical columns: {}\"\"\".format(data_clsf.shape[1], \n",
    "                                      data_clsf.shape[0],\n",
    "                                      len(data_clsf.select_dtypes('O').columns),\n",
    "                                      len(data_clsf.select_dtypes(['int', 'float']).columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we don't have any categorical columns. That's easy!!\n",
    "\n",
    "Let's have a descriptive statistical tables of the numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_clsf.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How's the distribution of numerical columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def plot_distribution(data, features):\n",
    "    i = 0\n",
    "    plt.figure()\n",
    "    col = 3\n",
    "    row = int(np.ceil(len(features)/col))\n",
    "    fig, ax = plt.subplots(row,col,figsize=(18,20))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "\n",
    "    for feature in features:\n",
    "        i += 1\n",
    "        plt.subplot(row,col,i);\n",
    "        sns.distplot(data[feature])\n",
    "        plt.xlabel(feature, fontsize=16)\n",
    "        locs, labels = plt.xticks()\n",
    "        plt.tick_params(axis='x', which='major', labelsize=12)\n",
    "        plt.tick_params(axis='y', which='major', labelsize=12)\n",
    "    plt.show();\n",
    "    \n",
    "plot_distribution(data_clsf, data_clsf.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OOHH MY GOOOOOSSHHH!!\n",
    "\n",
    "Very well distributed data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly check the correlations between the columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = data_clsf.corr()\n",
    "fig, ax = plt.subplots(figsize=(10,8))  \n",
    "sns.heatmap(corr, \n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values,\n",
    "            annot=True, fmt=\".2f\", cmap=\"YlGnBu\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there any **missing values**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(data_clsf.isnull().sum(), columns=['Missing Values'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DAMN!! We are very lucky!! Still we've got no missing values!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What percentage belongs to what ratings? Let's understand the rating distribution in our data.\n",
    "It is very important to check wheather your classification dataset is balanced or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_counts = data_clsf['quality'].value_counts()\n",
    "sizes = val_counts.values\n",
    "labels = val_counts.index\n",
    "\n",
    "class_dist = pd.DataFrame({\"rating\":labels,\n",
    "                           \"count\":sizes,\n",
    "                           \"percentage\": np.round((sizes/sum(sizes))*100, 2)})\n",
    "class_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_class_dist(sizes, labels):\n",
    "    # explode = [0.05]*len(val_counts.index)\n",
    "    fig1, ax1 = plt.subplots(figsize=(5,5))\n",
    "    ax1.pie(sizes, \n",
    "            labels=labels, \n",
    "            autopct='%1.1f%%', \n",
    "            startangle=90,\n",
    "    #         explode=explode,\n",
    "            pctdistance=0.85)\n",
    "\n",
    "    centre_circle = plt.Circle((0,0),0.50, fc='white')\n",
    "    fig = plt.gcf()\n",
    "    fig.gca().add_artist(centre_circle)\n",
    "\n",
    "    ax1.axis('equal')  \n",
    "    plt.tight_layout()\n",
    "    plt.show();\n",
    "    \n",
    "plot_class_dist(sizes, labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the data is not well distributed. It is called the **imbalanced class** problem. The dominant rating is **1=Good Quality**. To get a better accuracy from the model, we might need to balance the dataset using **sampling techniques (oversampling/undersampling)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time for **data segregation**. We need to separate the feature columns and target column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_clsf.loc[:, data_clsf.columns!='quality']\n",
    "y = data_clsf.loc[:, data_clsf.columns=='quality']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem, we don't need to normalize/standardize the feaures. Because wwe have very well-distributed features. And also we'll train a **tree based model** this time, **which is not affected by feature transformation**. \n",
    "\n",
    "So, let's jump to making the **traing and validation** dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, \n",
    "                                                    stratify=y, random_state=1234)\n",
    "\n",
    "print(\"\"\"\n",
    "X_train has {} data points.\n",
    "y_train has {} data points.\n",
    "X_test has {} data points.\n",
    "y_test has {} data points.\n",
    "\"\"\".format(X_train.shape[0], y_train.shape[0], X_test.shape[0], y_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WOOOHAAA!! \n",
    "\n",
    "Again, we've reached the modeling part! The easiest one!\n",
    "\n",
    "We'll use **Decision Tree** algorithm for data modeling. This is one of the most powerful yet an easy model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate decision trees, let’s listen to the story of Sajid.\n",
    "\n",
    "<img src=\"imgs/shopping_table.png\" alt=\"drawing\" width=\"500\">\n",
    "\n",
    "Here we can see the **amount of grocery items**, how was the **weather**, and whether Sajid went to **work** taht day. Green rows are days he went to the store and shopped the items, and red days are those he didn’t. \n",
    "\n",
    "This is how a decition tree will get trained.\n",
    "\n",
    "Let’s divide the first attribute up into a tree. Sajid can either have a low, medium, or high amount of items to shop:\n",
    "\n",
    "<img src=\"imgs/decision_tree_1.png\" alt=\"drawing\" width=\"500\">\n",
    "\n",
    "Here we can see that Sajid never shoped if he had a high amount of items in the list. This is called a pure subset, a subset with only positive or only negative examples. With decision trees, there is no need to break a pure subset down further.\n",
    "\n",
    "Let’s break the Med Items category into whether Sajid worked that day:\n",
    "\n",
    "<img src=\"imgs/decision_tree_2.png\" alt=\"drawing\" width=\"500\">\n",
    "\n",
    "Here we can see we have two more pure subsets, so this tree is complete. We can replace any pure subsets with their respective answer - in this case, yes or no.\n",
    "\n",
    "Finally, let’s split the Low Items category by the Weather attribute:\n",
    "\n",
    "<img src=\"imgs/decision_tree_3.png\" alt=\"drawing\" width=\"500\">\n",
    "\n",
    "Now that we have all pure subsets, we can create our final decision tree:\n",
    "\n",
    "<img src=\"imgs/decision_tree_4.png\" alt=\"drawing\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best thing about Decision Tree is you can extract the **feature importance** from the model and also interpret the tree for **extracting the underlying patterns** in data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's train a Decision Tree to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf_model = DecisionTreeClassifier(random_state=1234)\n",
    "clf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are wide variety of evaluation metrics for validating regression models such as **Accuracy, AUC, Precision, Recall, F1 score**. For our problem we'll use the **accuracy** metric.\n",
    "\n",
    "**Remember:** Accuracy is not always the best metric to valiate classification model. Sometime, depending on the problem, you'll need to choose other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc_score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"The accuracy score is {}\".format(acc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make prediction on live data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "live_data_clsf = pd.DataFrame({\n",
    "  'fixed acidity': 7.4,\n",
    "  'volatile acidity': 0.7,\n",
    "  'citric acid': 0.0,\n",
    "  'residual sugar': 1.9,\n",
    "  'chlorides': 0.076,\n",
    "  'free sulfur dioxide': 11.0,\n",
    "  'total sulfur dioxide': 34.0,\n",
    "  'density': 0.9978,\n",
    "  'pH': 3.51,\n",
    "  'sulphates': 0.56,\n",
    "  'alcohol': 9.4\n",
    "}, index = [0])\n",
    "\n",
    "live_data_clsf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make prediction using the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_live = clf_model.predict(live_data_clsf)\n",
    "print(y_pred_live)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### Get the Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coef = pd.Series(clf_model.feature_importances_, index = X.columns)\n",
    "imp_coef = coef.sort_values(ascending=False)\n",
    "\n",
    "def plot_importance(feat_imp, feat_name):\n",
    "    fig, ax = plt.subplots(figsize=(8,5)) \n",
    "    sns.set(style=\"darkgrid\", context=\"poster\")\n",
    "    sns.barplot(feat_imp, feat_name, palette=\"deep\")\n",
    "    plt.title(\"Feature importance\", fontsize=14)\n",
    "    plt.tick_params(axis='x', labelsize=13)\n",
    "    plt.tick_params(axis='y', labelsize=13)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "plot_importance(imp_coef.values, imp_coef.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a visualization of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If you don't have pydotplus installed in your PC, please run the following command\n",
    "# !pip install pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import pydotplus\n",
    "\n",
    "dot_data = tree.export_graphviz(clf_model,\n",
    "                                feature_names=X_train.columns,\n",
    "                                out_file=None,\n",
    "                                filled=True,\n",
    "                                rounded=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "graph.write_svg('tree.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='case3'></a>\n",
    "# Case Study 3: Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Applications of Clustering Algorithm:**\n",
    "* Market/Customer Segmentation\n",
    "* Behavioural Segmentation\n",
    "* Anomaly Detection\n",
    "* Social Network Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "<h3 style=color:navy>Problem Description:</h3>\n",
    "\n",
    "In this project, we will analyze a dataset containing data on various customers’ annual spending amounts (reported in monetary units) of diverse product categories for internal structure. One goal of this project is to best describe the variation in the different types of customers that a wholesale distributor interacts with. Doing so would equip the distributor with insight into how to best structure their delivery service to meet the needs of each customer.\n",
    "\n",
    "\n",
    "\n",
    "#### Let's find the customer segments!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_clst = pd.read_csv(\"data/clustering_wholesale_customers.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Exploration (Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a glance of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_clst.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the data shape and datatypes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "No of columns: {}\n",
    "No of rows: {}\n",
    "No of categorical columns: {}\n",
    "No of numerical columns: {}\"\"\".format(data_clst.shape[1], \n",
    "                                      data_clst.shape[0],\n",
    "                                      len(data_clst.select_dtypes('O').columns),\n",
    "                                      len(data_clst.select_dtypes(['int', 'float']).columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we don't have any categorical columns. That's easy!!\n",
    "\n",
    "Let's have a descriptive statistical tables of the numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_clst.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How's the distribution of numerical columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def plot_distribution(data, features):\n",
    "    i = 0\n",
    "    plt.figure()\n",
    "    col = 3\n",
    "    row = int(np.ceil(len(features)/col))\n",
    "    fig, ax = plt.subplots(row,col,figsize=(18,12))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "\n",
    "    for feature in features:\n",
    "        i += 1\n",
    "        plt.subplot(row,col,i);\n",
    "        sns.distplot(data[feature])\n",
    "        plt.xlabel(feature, fontsize=16)\n",
    "        locs, labels = plt.xticks()\n",
    "        plt.tick_params(axis='x', which='major', labelsize=12)\n",
    "        plt.tick_params(axis='y', which='major', labelsize=12)\n",
    "    plt.show();\n",
    "    \n",
    "plot_distribution(data_clst, data_clst.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well distributed data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly check the correlations between the columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = data_clst.corr()\n",
    "fig, ax = plt.subplots(figsize=(8,6))  \n",
    "sns.heatmap(corr, \n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values,\n",
    "            annot=True, fmt=\".2f\", cmap=\"RdBu_r\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there any **missing values**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(data_clst.isnull().sum(), columns=['Missing Values'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DAMN!! We are very lucky!! No missing values!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait, this is a unsupervised clustering problem. So we don't need any **data segregation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering algorithms **calculates distance from the centroids**. So, it is **highly affective to scaling**. Now, we need to perform scaling on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data_clst)\n",
    "data_clst_scaled = scaler.transform(data_clst.values)\n",
    "data_clst_scaled = pd.DataFrame(data_clst_scaled, columns=data_clst.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the modeling part!\n",
    "\n",
    "In this section, we will use a **K-Means** clustering algorithm to identify the various customer segments hidden in the data. We will then recover specific data points from the clusters to understand their significance by transforming them back into their original dimension and scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K means is an iterative clustering algorithm that aims to find local maxima in each iteration. This algorithm works in these 5 steps :\n",
    "\n",
    "**1. Specify the desired number of clusters K:** Let us choose k=2 for these 5 data points in 2-D space.\n",
    "\n",
    "<img src=\"imgs/kmc-1.png\" alt=\"drawing\" width=\"200\">\n",
    "\n",
    "**2. Randomly assign each data point to a cluster:** Let’s assign three points in cluster 1 shown using red color and two points in cluster 2 shown using grey color.\n",
    "\n",
    "<img src=\"imgs/kmc-2.png\" alt=\"drawing\" width=\"200\">\n",
    "\n",
    "**3. Compute cluster centroids:** The centroid of data points in the red cluster is shown using red cross and those in grey cluster using grey cross.\n",
    "\n",
    "<img src=\"imgs/kmc-3.png\" alt=\"drawing\" width=\"200\">\n",
    "\n",
    "**4. Re-assign each point to the closest cluster centroid:** Note that only the data point at the bottom is assigned to the red cluster even though its closer to the centroid of grey cluster. Thus, we assign that data point into grey cluster.\n",
    "\n",
    "<img src=\"imgs/kmc-4.png\" alt=\"drawing\" width=\"200\">\n",
    "\n",
    "**5. Re-compute cluster centroids:** Now, re-computing the centroids for both the clusters.\n",
    "\n",
    "<img src=\"imgs/kmc-5.png\" alt=\"drawing\" width=\"200\">\n",
    "\n",
    "**6. Repeat steps 4 and 5 until no improvements are possible:** Similarly, we’ll repeat the $4th$ and $5th$ steps until we’ll reach global optima. When there will be no further switching of data points between two clusters for two successive repeats. It will mark the termination of the algorithm if not explicitly mentioned.\n",
    "\n",
    "**Advantages of K-Means clustering**:\n",
    "* Simple, easy to implement and interpret results.\n",
    "* Good for hard cluster assignments i.e. when a data point only belongs to one cluster over the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's train a K-Means clustering to our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we need to find the number of $k$ clusters. How can we determine the value of $k$?\n",
    "\n",
    "Using **elbow method**!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def plot_elbow(data):\n",
    "    sum_of_squared_distances = []\n",
    "    K = range(1,10)\n",
    "    for k in K:\n",
    "        km = KMeans(n_clusters=k, random_state=1234)\n",
    "        km = km.fit(data)\n",
    "        sum_of_squared_distances.append(km.inertia_)\n",
    "\n",
    "    plt.figure(figsize=(6,4))    \n",
    "    sns.set(style=\"darkgrid\")\n",
    "    plt.plot(K, sum_of_squared_distances, 'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.xlim(0,10)\n",
    "    plt.xticks(np.arange(0,11,1))\n",
    "    plt.ylabel('Sum of squared distances')\n",
    "    plt.title('Elbow Method For Optimal k')\n",
    "    plt.show();\n",
    "\n",
    "plot_elbow(data_clst_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, train the clustering model using the determined value of $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(n_clusters=2)\n",
    "km.fit(data_clst_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Validation (not actually; it may named as inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = km.predict(data_clst_scaled)\n",
    "labels = km.labels_\n",
    "centers = km.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this is unspervised learning, we can not calculate accuracy or prediction errors. However, we can determine the quality of clustring by calculating Silhouette Score.\n",
    "\n",
    "**The score is bounded between -1 for incorrect clustering and +1 for highly dense clustering. Scores around zero indicate overlapping clusters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "silhouette = silhouette_score(data_clst, labels, metric='euclidean')\n",
    "\n",
    "print(silhouette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "true_centers = scaler.inverse_transform(centers)\n",
    "\n",
    "segments = ['Segment {}'.format(i+1) for i in range(0,len(true_centers))]\n",
    "true_centers = pd.DataFrame(np.round(true_centers), columns=data_clst.keys())\n",
    "true_centers.index = segments\n",
    "\n",
    "true_centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, let's make inference from the segments**:\n",
    "\n",
    "* **Segment 1**: This segment best represents retailers. Their spend on Fresh is the highest. Their spend on other categories are lower but close to mean and median.\n",
    "\n",
    "* **Segment 2**: This segment best represents supermarkets. They spend the higher than mean and median amount on Milk, Grocery, Frozen, Detergents_Paper and Delicassen, which are both essential to be stocked in such places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clst.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data_clst[preds==0]['Fresh'], data_clst[preds==0]['Milk'], \n",
    "            c='navy', s=50, label='Segment 1')\n",
    "\n",
    "plt.scatter(data_clst[preds==1]['Fresh'], data_clst[preds==1]['Milk'], \n",
    "            c='orange', s=50, label='Segment 2')\n",
    "\n",
    "plt.scatter(true_centers['Fresh'], \n",
    "            true_centers['Grocery'],\n",
    "            s=50, marker='s', c='red',\n",
    "            alpha=0.7, label='Centroids')\n",
    "\n",
    "plt.title('Customer segments')\n",
    "plt.xlabel('Sales on Fresh')\n",
    "plt.ylabel('Sales on Milk')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(1, figsize=(7,7))\n",
    "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
    "ax.scatter(data_clst['Fresh'], \n",
    "           data_clst['Milk'], \n",
    "           data_clst['Grocery'],\n",
    "           c=labels.astype(np.float), \n",
    "           edgecolor=\"k\", s=50)\n",
    "\n",
    "ax.set_xlabel(\"Sales on Fresh\")\n",
    "ax.set_ylabel(\"Sales on Milk\")\n",
    "ax.set_zlabel(\"Sales on Grocery\")\n",
    "plt.title(\"Customer segments\", fontsize=14)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='resources'></a>\n",
    "## Learning Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Machine Learning](https://www.coursera.org/learn/machine-learning)\n",
    "* [Applied Data Science with Python Specialization](https://www.coursera.org/specializations/data-science-python)\n",
    "* [Mathematics for Machine Learning Specialization](https://www.coursera.org/specializations/mathematics-machine-learning)\n",
    "* [Data Science Essentials](https://www.edx.org/course/data-science-essentials)\n",
    "* [Intro to Data Science](https://www.udacity.com/course/intro-to-data-science--ud359)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='research'></a>\n",
    "## Conducting research in Applied Data Science and Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Data Science research can be categorized into two types:\n",
    "* **Pure Research:** You need to bring innovation and algorithmic change.\n",
    "* **Applied research:** You need to apply ML methods to solve innovative/impactful problems.\n",
    "\n",
    "### Research articles can be (basically) categorized into three types:\n",
    "* **Journal Paper:** The articles are long (typically 10+ pages). Very rigorous and long review periods. They usually expect innovation and algorithmic development. Some journals asks for article processing fees.\n",
    "* **Review Paper:** You need to find and analyze hundreds of papers from previous 5/10/15 years on your topic of interest. Critically analyze their approaches and summerize and criticize. You need to be very strong in english.\n",
    "* **Conference Paper:** This is the shorter version. Most of the conference papers accepts applied research works. Typically bouned to 4-6 pages. After draft paper submission, your paper will have to pass the review process (1-2 months). If your paper get the acceptance, you'll need to attend the confernce and present your paper. The conference registration fee is a must.\n",
    "\n",
    "### How to choose good journals and conferences?\n",
    "* Indexing: Scopus, IEEE, ACM, EI, ISI, DBLP\n",
    "* Quartiles, Impact Factors and H-Index\n",
    "* Age of the conference/journals\n",
    "\n",
    "### Important Websites\n",
    "* https://www.scimagojr.com/\n",
    "* http://dblp.org\n",
    "* http://wikicfp.com\n",
    "* http://guide2research.com\n",
    "* https://researchgate.net\n",
    "\n",
    "### Popular conferences for Data Science & Machine Learning\n",
    "* [SIGKDD](https://www.kdd.org/kdd2019/): ACM SIGKDD Conference on Knowledge Discovery and Data Mining\t\n",
    "* [ICDE](http://conferences.cis.umac.mo/icde2019/):\tInternational Conference on Data Engineering\n",
    "* [ICDM](http://icdm2018.org):\tInternational Conference on Data Mining\n",
    "* [CKIM](http://www.cikm2019.net):\tInternational Conference on Information and Knowledge Management\n",
    "* [ICML](https://icml.cc):\tInternational Conference on Machine Learning\n",
    "* [NeurIPS (NIPS)](https://nips.cc/):\tConference on Neural Information Processing Systems\n",
    "* [CVPR](https://www.thecvf.com/):\tConference on Computer Vision and Pattern Recognition\n",
    "* [AAAI](https://aaai.org/Conferences/AAAI/aaai.php):\tAssociation for the Advancement of Artificial Intelligence\n",
    "* [ICLR](https://iclr.cc/):\tInternational Conference on Learning Representations\n",
    "\n",
    "If you are interested in applied research in Data Science and Machine Learning, feel free to knock me (abdalimran@gmail.com). I can supervise your research projects. I'm always looking for dedicated collaborators and research groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Good luck future Data Scientists!!</h1></center>\n",
    "\n",
    "<img src=\"imgs/bol.gif\" alt=\"drawing\" width=\"400\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
